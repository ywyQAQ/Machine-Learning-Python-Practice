# 数据特征选定

在做数据挖掘和数据分析时，数据是所有问题的基础，并且会影响整个项目的进程。相较于使用一些复杂的算法，灵活地处理数据经常会取到意想不到的效果。而处理数据不可避免地会使用到特征工作。那么特征工程是什么呢？有这么一句话在业界广为流传：**数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已**。因此，特征工程的本质就是一项工程活动，目的是最大限度地从原始数据中提取合适的特征，以供算法和模型使用。特征处理是特征工程的核心部分，scikit-learn提供了较为完整的特征处理方法，包括数据预处理、特征选择、降维等。

本章将介绍以下四个数据特征选择的方法:

- 单变量特征选定
- 递归特征消除
- 主要成分分析
- 特征的重要性

## 1.特征选定

特征选定是一个流程，能够选择有助于提高预测结果准确度的特征数据，或者有助于发现我们感兴趣的输出结果的特征数据。如果数据中包含无关的特征属性，会降低算法的准确度，对预测新数据造成干扰，尤其是线性相关算法（如线性回归算法和逻辑回归算法）。

- 降低数据的拟合度：较少的冗余数据，会使算法得出结论的机会更大。
- 提高算法精度：较少的误导数据，能够提高算法的准确度。
- 减少训练时间：越少的数据，训练模型所需要的事件越少。

## 2.单变量特征选定

统计分析可以用来分析选择**对结果影响最大的数据特征**。在scikit-learn中提供了SelectKBest类，可以使用一系列统计方法来选定数据特征，是对卡方检验的实现。经典的卡方检验是检验定性自变量对定性因变量的相关性的方法。假设自变量有N种取值，因变量有M种取值，考虑自变量等于 `i` 且因变量等于 `j` 的样本频数的观察值与期望值的差距，构建统计量。卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，偏离程度决定了卡方值的大小，卡方值越大，约不符合；卡方值越小，偏差越小，越趋于符合；若两个值完全相等，卡方值就为0，表明理论值完全符合。下面的例子是通过卡方检验（chi-squared）的方式来选择四个对结果影响最大的数据特征。代码如下：

```python
from pandas import read_csv
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2


# 导入数据
    filename = 'pima_data.csv'
    names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
    data = read_csv(filename, names=names)
    # 将数据分为输入数据和输出结果
    array = data.values
    X = array[:, 0:8]
    Y = array[:, 8]
    # 特征选定
    test = SelectKBest(score_func=chi2,k=4)
    fit = test.fit(X, Y)
    set_printoptions(precision=3)
    print(fit.scores_)
    features = test.fit_transform(X, Y)
    print(features)
```

```
[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]
[[148.    0.   33.6  50. ]
 [ 85.    0.   26.6  31. ]
 [183.    0.   23.3  32. ]
 ...
 [121.  112.   26.2  30. ]
 [126.    0.   30.1  47. ]
 [ 93.    0.   30.4  23. ]]
```

## 3.递归特征消除

递归特征消除（RFE）使用一个基模型来进行多轮训练，每轮训练后消除若干权值系数的特征，再基于新的特征集进行下一轮训练。通过每一个基模型的精度，找到对最终的预测结果影响最大的数据特征。下面的例子是以逻辑回归算法为基模型，通过递归特征消除来选定对预测结果影响最大的三个数据特征。代码如下：

```python
from pandas import read_csv
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
   
   # 导入数据
    filename = 'pima_data.csv'
    names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
    data = read_csv(filename, names=names)
    # 将数据分为输入数据和输出结果
    array = data.values
    X = array[:, 0:8]
    Y = array[:, 8]
    # 特征选定
    model = LogisticRegression()
    rfe = RFE(model, 3)
    fit = rfe.fit(X, Y)
    print("特征个数：")
    print(fit.n_features_)
    print('被选定的特征：')
    print(fit.support_)
    print('特征排名：')
    print(fit.ranking_)
```

```
特征个数：
3
被选定的特征：
[ True False False False False  True  True False]
特征排名：
[1 2 4 5 6 1 1 3]
```

执行后，我们可以看到RFE选定了preg、mass和pedi三个数据特征，它们在`support_`中被标记为True，在`ranking_`中被标记为1。

## 4.主要成分分析

主要成分分析（PCA）是使用线性代数来转换压缩数据，通常被称作数据降维。常见的降维方法除了主要成分分析（PCA），还有线性判别分析（LDA），它本身也是一个分类模型。PCA和LDA有很多的相似之处，其本质是将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说，PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。在聚类算法中，通常会利用PCA对数据进行降维处理，以利于对数据的简化分析和可视化。

```
from pandas import read_csv
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

    # 导入数据
    filename = 'pima_data.csv'
    names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
    data = read_csv(filename, names=names)
    # 将数据分为输入数据和输出结果
    array = data.values
    X = array[:, 0:8]
    Y = array[:, 8]
    # 特征选定
    pca = PCA(n_components=3)
    fit = pca.fit(X)
    print("解释方差： %s" % fit.explained_variance_ratio_)
    print(fit.components_)
```

执行结果如下：

```
解释方差： [0.88854663 0.06159078 0.02579012]
[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02
   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]
 [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02
   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]
 [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01
   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]
```

## 5.特征重要性

袋装决策树算法（Bagged Decision Trees）、随机森林算法和极端随机树算法都可以用来计算数据特征的重要性。这三个算法都是集成算法中的袋装算法，在后面的集成算法章节会有详细的介绍。下面给出一个使用ExtraTreesClassifier类进行特征的重要性计算的例子。代码如下：

```python
from pandas import read_csv
from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import RFE
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
  # 导入数据
    filename = 'pima_data.csv'
    names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
    data = read_csv(filename, names=names)
    # 将数据分为输入数据和输出结果
    array = data.values
    X = array[:, 0:8]
    Y = array[:, 8]
    # 特征选定
    model = ExtraTreesClassifier()
    fit = model.fit(X, Y)
    print(fit.feature_importances_)
```

我们可以看到算法给出了每个数据特征的得分。

```
[0.10747662 0.24258543 0.0969691  0.08074547 0.07532837 0.13544405
 0.11805656 0.1433944 ]
```

## 6.总结

接下来介绍通过采样数据来评估算法模型的方法
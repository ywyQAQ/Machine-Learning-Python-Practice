# 数据理解

为了得到更准确的结果，必须理解数据的特征、分布情况，以及需要解决的问题，以便建立和优化算法模型。本章将介绍七种方法来帮助大家理解数据。

- 简单地查看数据。
- 审查数据的维度。
- 审查数据的类型和属性。
- 总结查看数据分类的分布情况。
- 通过描述性统计分析数据。
- 理解数据属性的相关性。
- 审查数据的分布状况。

## 1.简单地查看数据

下面通过一个例子展示一下如何查看数据。这个例子是查看前10行数据。代码如下：

```python
from pandas import read_csv

# 显示数据的前10行
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
peek = data.head(10)
print(peek)
```

执行结果如下：

```
   preg  plas  pres  skin  test  mass   pedi  age  class
0     6   148    72    35     0  33.6  0.627   50      1
1     1    85    66    29     0  26.6  0.351   31      0
2     8   183    64     0     0  23.3  0.672   32      1
3     1    89    66    23    94  28.1  0.167   21      0
4     0   137    40    35   168  43.1  2.288   33      1
5     5   116    74     0     0  25.6  0.201   30      0
6     3    78    50    32    88  31.0  0.248   26      1
7    10   115     0     0     0  35.3  0.134   29      0
8     2   197    70    45   543  30.5  0.158   53      1
9     8   125    96     0     0   0.0  0.232   54      1
```

## 2.数据的维度

在机器学习中要注意数据的行和列，必须对所拥有的数据非常了解，要知道有多少行和多少列，这是因为：

- 太多的行会导致花费大量时间来训练算法得到模型；太少的数据会导致对算法的训练不充分，得不到合适的模型。
- 如果数据具有太多特征，会引起某些算法性能低下的问题。

通过DataFrame的shape属性，可以很方便地查看数据集总有多少行和多少列。代码如下：

```python
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
print(data.shape)
```

执行结果如下：

```
(768, 9)
```

## 3.数据属性和类型

数据的类型是很重要的一个属性。字符串会被转化成浮点数或证书，以便于计算机分类。可以通过DataFrame的Type属性来查看每一个字段的数据类型，代码如下：

```python
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
print(data.dtypes)
```

执行结果如下：

```
preg       int64
plas       int64
pres       int64
skin       int64
test       int64
mass     float64
pedi     float64
age        int64
class      int64
dtype: object
```

## 4.描述性统计

描述性统计可以给出一个更加直观、更加清晰的视角，以加强对数据的理解。在这里可以通过DataFrame的describe()方法查看描述性统计的内容。这个方法给我们展示了八方面的信息：数据记录数、平均值、标准方差、最小值、下四分位数、中位数、上四分位数、最大值。这些信息主要用来描述数据的分布情况。代码如下：

```python
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
set_option('display.width', 100)
# 设置数据的精确度
set_option('precision', 4)
print(data.describe())
```

执行结果如下：

```
           preg      plas      pres      skin      test      mass      pedi       age    class
count  768.0000  768.0000  768.0000  768.0000  768.0000  768.0000  768.0000  768.0000  768.000
mean     3.8451  120.8945   69.1055   20.5365   79.7995   31.9926    0.4719   33.2409    0.349
std      3.3696   31.9726   19.3558   15.9522  115.2440    7.8842    0.3313   11.7602    0.477
min      0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0780   21.0000    0.000
25%      1.0000   99.0000   62.0000    0.0000    0.0000   27.3000    0.2437   24.0000    0.000
50%      3.0000  117.0000   72.0000   23.0000   30.5000   32.0000    0.3725   29.0000    0.000
75%      6.0000  140.2500   80.0000   32.0000  127.2500   36.6000    0.6262   41.0000    1.000
max     17.0000  199.0000  122.0000   99.0000  846.0000   67.1000    2.4200   81.0000    1.000

```

## 5.数据分组分布（适用于分类算法）

在分类算法中，需要知道每个分类的数据大概有多少条记录，以及数据分布是否平衡。如果数据分布的平衡性很差，需要在家具加工阶段进行数据处理，来提高数据分布的平衡性。利用Pandas的属性和方法，可以很方便地查看数据的分布情况。代码如下：

```python
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
print(data.groupby('class').size())
```

```
class
0    500
1    268
dtype: int64
```

## 6.数据属性的相关性

数据属性的相关性就是指数据的两个属性是否相互影响，以及这种影响是什么方式的等。非常通用的计算两个属性的相关性的方法是皮尔逊相关系数，皮尔逊相关系数是度量两个变量相关程度的方法。它是一个介于1和-1之间的值，其中，1表示变量完全正相关，0表示无关，-1表示完全负相关。在机器学习中，当数据的关联性比较高时，有些算法（如linear、逻辑回归算法等）的性能会降低。所以在开始训练算法之前，查看一下算法的关联性是一个很好的方法。当数据特征的相关性比较高时，应该考虑对特征进行降维处理。下面通过`DataFrame`的`corr()`方法来计算数据集中数据属性之间的关联关系矩阵。

```python
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
set_option('display.width', 100)
# 设置数据的精确度
set_option('precision', 2)
print(data.corr(method='pearson'))
```

```
       preg  plas  pres  skin  test  mass  pedi   age  class
preg   1.00  0.13  0.14 -0.08 -0.07  0.02 -0.03  0.54   0.22
plas   0.13  1.00  0.15  0.06  0.33  0.22  0.14  0.26   0.47
pres   0.14  0.15  1.00  0.21  0.09  0.28  0.04  0.24   0.07
skin  -0.08  0.06  0.21  1.00  0.44  0.39  0.18 -0.11   0.07
test  -0.07  0.33  0.09  0.44  1.00  0.20  0.19 -0.04   0.13
mass   0.02  0.22  0.28  0.39  0.20  1.00  0.14  0.04   0.29
pedi  -0.03  0.14  0.04  0.18  0.19  0.14  1.00  0.03   0.17
age    0.54  0.26  0.24 -0.11 -0.04  0.04  0.03  1.00   0.24
class  0.22  0.47  0.07  0.07  0.13  0.29  0.17  0.24   1.00
```

## 7.数据的分布分析

通过分析数据的高斯分布情况来确认数据的偏离情况。高斯分布又叫正态分布，是在数据、物理及工程等领域都非常重要的概率分布。高斯分布的曲线呈钟形，两头低，中间高，左右对称。在很多机器学习中，都会假定数据遵循高斯分布，先计算数据的高斯偏离状况，再根据偏离状况准备数据。我们可以使用`DataFrame`的`skew()`方法来计算所有数据属性的高斯分布偏离情况。

```python
filename = 'pima_data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
print(data.skew())
```

skew()函数的结果显示了数据分布是左偏还是右偏，当数据接近0时，表示数据的偏差非常小。

```
preg     0.901674
plas     0.173754
pres    -1.843608
skin     0.109372
test     2.272251
mass    -0.428982
pedi     1.919911
age      1.129597
class    0.635017
dtype: float64
```

## 8.总结

本章学了如何分析和理解数据。

- **审查数据**：
- **问为什么**：
- **写下想法**：